---
title: High Availability
description: PodDisruptionBudget, HorizontalPodAutoscaler, and rolling updates
navigation:
  icon: i-lucide-shield-check
---

Configure high availability features for production deployments.

## PodDisruptionBudget (PDB)

PDB ensures availability during voluntary disruptions (node drains, cluster upgrades, scaling down).

::warning{title="Requires Multiple Replicas"}
PDB only works with 2+ replicas. Single-replica deployments cannot benefit from PDB.
::

### Enable PDB

```yaml
heimdall:
  podDisruptionBudget:
    enabled: true
    minAvailable: 1  # At least 1 pod must remain available
```

### Example Scenarios

- **Node Drain**: Kubernetes ensures 1 pod stays running while draining
- **Cluster Upgrade**: Rolling upgrade won't take down all pods at once
- **Scaling Down**: Prevents accidental removal of all pods

### Alternative Configuration (maxUnavailable)

```yaml
heimdall:
  podDisruptionBudget:
    enabled: true
    maxUnavailable: 1  # Allow at most 1 pod to be unavailable
```

::caution{title="Cannot Use Both"}
Specify either `minAvailable` OR `maxUnavailable`, not both.
::

### Verify PDB

Check PDB status:
```bash
kubectl get pdb -n heimdall
```

Expected output:
```
NAME       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
heimdall   1               N/A               1                     5m
```

Test PDB (drain node):
```bash
# Drain node (PDB prevents disruption if only 1 pod)
kubectl drain <node-name> --ignore-daemonsets

# PDB blocks drain until another pod is ready
```

## HorizontalPodAutoscaler (HPA)

HPA automatically scales pods based on resource utilization.

### Prerequisites

- metrics-server installed in cluster
- Resource requests defined in values.yaml

### Enable HPA

```yaml
heimdall:
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80

  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
```

### How HPA Works

1. metrics-server collects CPU/memory usage every 15s
2. HPA calculates: `desiredReplicas = ceil(currentReplicas × (currentUtilization / targetUtilization))`
3. If desiredReplicas differs from current, HPA scales up/down
4. Respects minReplicas and maxReplicas bounds

### Scaling Example

```yaml
heimdall:
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70

# Example: CPU usage reaches 85%
# desiredReplicas = ceil(2 × (85 / 70)) = ceil(2.43) = 3
# HPA scales from 2 → 3 pods
```

### Verify HPA

Check HPA status:
```bash
kubectl get hpa -n heimdall
```

Expected output:
```
NAME       REFERENCE             TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
heimdall   StatefulSet/heimdall  45%/70%   2         10        2          5m
```

Watch HPA scaling:
```bash
kubectl get hpa -n heimdall -w
```

### Test Autoscaling

Generate load:
```bash
# Port-forward to Heimdall
kubectl port-forward -n heimdall heimdall-0 3000:3000 &

# Generate load with Apache Bench
ab -n 10000 -c 100 http://localhost:3000/

# Watch HPA scale up
kubectl get hpa -n heimdall -w
```

## Rolling Updates

The chart uses StatefulSet with RollingUpdate strategy for zero-downtime deployments.

### Default Configuration

```yaml
# Configured automatically in StatefulSet spec
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    partition: 0  # Update all pods
```

### How It Works

1. **Sequential Updates**: Pods update one at a time in reverse ordinal order (pod-2, pod-1, pod-0)
2. **Health Checks**: Each pod must pass readiness probe before next pod updates
3. **Automatic Rollback**: Failed updates stop automatically (pod remains unhealthy)

### Update Flow

```
Old Version:  pod-0 ✓  pod-1 ✓  pod-2 ✓
              ↓       ↓       ↓
Update:       pod-0 ✓  pod-1 ✓  pod-2 → updating → ready ✓
              ↓       ↓
Update:       pod-0 ✓  pod-1 → updating → ready ✓
              ↓
Update:       pod-0 → updating → ready ✓
```

### Partitioned Updates (Canary Deployments)

Update only pods with ordinal >= 2 (useful for testing):

```yaml
updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    partition: 2
```

::note
StatefulSet updates maintain pod identity and storage. Each pod retains its PVC across updates.
::

## Combining PDB and HPA

For production deployments, use both:

```yaml
heimdall:
  # Auto-scaling
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70

  # Disruption protection
  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  # Resource requests (required for HPA)
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2000m
      memory: 2Gi
```

### Behavior

- HPA scales between 2-10 pods based on load
- PDB ensures at least 1 pod always available during disruptions
- Even at min (2 pods), one can be disrupted while other handles traffic

## Best Practices

### Development

```yaml
heimdall:
  podDisruptionBudget:
    enabled: false  # Not needed for dev

  autoscaling:
    enabled: false  # Fixed replicas for dev
```

### Staging

```yaml
heimdall:
  podDisruptionBudget:
    enabled: true
    minAvailable: 1

  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 5
```

### Production

```yaml
heimdall:
  podDisruptionBudget:
    enabled: true
    minAvailable: 2  # Ensure 2 pods always available

  autoscaling:
    enabled: true
    minReplicas: 3  # Start with 3 for redundancy
    maxReplicas: 20  # Scale aggressively
    targetCPUUtilizationPercentage: 60  # Scale earlier

  resources:
    requests:
      cpu: 1000m
      memory: 1Gi
    limits:
      cpu: 4000m
      memory: 4Gi
```

## Future Improvements

Future chart versions may support:

- Custom metrics (requests per second, queue depth)
- External metrics (cloud load balancer metrics)
- Multiple metrics (CPU AND memory)
- Scaling policies (faster scale-up, slower scale-down)

## See Also

- [Health Probes](/helm-chart/advanced/availability/health-probes) - Configure probes
- [Troubleshooting](/helm-chart/advanced/availability/troubleshooting) - Common issues
- [Database Configuration](/helm-chart/getting-started/database) - PostgreSQL setup
